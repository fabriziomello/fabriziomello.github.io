<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Postgresql | Fabrízio de Royes Mello]]></title>
  <link href="http://fabriziomello.github.io/blog/categories/postgresql/atom.xml" rel="self"/>
  <link href="http://fabriziomello.github.io/"/>
  <updated>2014-05-02T02:27:24-03:00</updated>
  <id>http://fabriziomello.github.io/</id>
  <author>
    <name><![CDATA[Fabrízio de Royes Mello]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Aguardando Pela 9.4 - Introduzindo JSONB]]></title>
    <link href="http://fabriziomello.github.io/blog/2014/05/01/aguardando-pela-9-dot-4-introduzindo-jsonb/"/>
    <updated>2014-05-01T23:59:07-03:00</updated>
    <id>http://fabriziomello.github.io/blog/2014/05/01/aguardando-pela-9-dot-4-introduzindo-jsonb</id>
    <content type="html"><![CDATA[<p><strong>ATENÇÃO!</strong> Este post é uma tradução para PT-BR do <a href="http://www.depesz.com/2014/03/25/waiting-for-9-4-introduce-jsonb-a-structured-format-for-storing-json/">blog do Sr. Hubert Depesz Lubaczewski</a>. Fique a vontade para comentar quaisquer problemas na tradução.</p>

<p>No dia 23 de Março de 2014, Andrew Dunstan <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commitdiff;h=d9134d0a355cfa447adc80db4505d5931084278a">commitou</a> o seguinte patch:</p>

<blockquote><p><strong>Introduzindo jsonb, um formato estruturado para armazenar json.</strong></p>

<p>O novo formato aceita exatamente os mesmos dados como o tipo json já existente.
Entretanto este é armazenado em um formato que não requer a realização de um parse
do text original para processá-lo, tornando-o muito mais adequado para indexação e
outras operações. Espaços em brancos desnecessários são descartados, e a ordem das
chaves dos objetos não é preservada. Nem são mantidas chaves de objetos duplicadas &ndash;
o valor para uma determinada chave é armazenada uma única vez.</p>

<p>O novo tipo possui todas as funções e operadores que o tipo json tem, com excessão
das funções para geração de objetos json (to_json, json_agg, etc.), e também possui
a mesma semântica. Adicionalmente existem classes de operadores para índices to tipo
hash e btree, e duas classes para índices GIN o qual não existe para o tipo json.</p>

<p>Essa funcionalidade evoluiu de um trabalho anterior do Oleg Bartunov e Teodor Sigaev,
que se destinava a fornecer facilidades semelhantes ao hstore aninhado, mas que no
final provou ter alguns problemas de compatibilidade significativos.</p>

<p>Autores: Oleg Bartunov, Teodor Sigaev, Peter Geoghegan e Andrew Dunstan.
Revisão: Andres Freund</p></blockquote>

<p>Depois que ele foi commitado, ele foi explicado muito bem, mas eu decidi escrever sobre isso também, com alguns exemplos. Primeiro, vamos ver como ele funciona.</p>

<p>Vou começar com alguns valores de teste:</p>

<pre><code>{"a":"abc","d":"def","z":[1,2,3]}

{"a":"abc","d";"def","z":[1x2,3]}

{
    "a": "abc",
    "d": "def",
    "z": [1, 2, 3]
}

{"a":"abc","d":"def","z":[1,2,3],"d":"overwritten"}
</code></pre>

<p>Primeiro, vamos ver o que acontece após transformar esses valores em json e jsonb:</p>

<pre><code>select '{"a":"abc","d":"def","z":[1,2,3]}'::json;
               json                
-----------------------------------
 {"a":"abc","d":"def","z":[1,2,3]}
(1 row)

select '{"a":"abc","d":"def","z":[1,2,3]}'::jsonb;
                  jsonb                   
------------------------------------------
 {"a": "abc", "d": "def", "z": [1, 2, 3]}
(1 row)
</code></pre>

<p>Tudo parece correto aqui, porém a saída do jsonb foi reformatada. Aqui ele não fez muita coisa, mas adicionou alguns espaços em branco. Isso é bom.</p>

<pre><code>select '{"a":"abc","d";"def","z":[1x2,3]}'::json;
ERROR:  invalid input syntax for type json
LINE 1: select '{"a":"abc","d";"def","z":[1x2,3]}'::json;
               ^
DETAIL:  Token ";" is invalid.
CONTEXT:  JSON data, line 1: {"a":"abc","d";...

select '{"a":"abc","d";"def","z":[1x2,3]}'::jsonb;
ERROR:  invalid input syntax for type json
LINE 1: select '{"a":"abc","d";"def","z":[1x2,3]}'::jsonb;
               ^
DETAIL:  Token ";" is invalid.
CONTEXT:  JSON data, line 1: {"a":"abc","d";...
</code></pre>

<p>Em ambos os casos o erro foi reportado corretamente, mas no caso do jsonb ele disse: &ldquo;invalid input syntax for type json&rdquo;. É provavelmente devido a ordem do cast, e normalmente isso deve estar correto. De qualquer maneira JSON e JSONB são similares o suficiente para não causar problemas.</p>

<pre><code>select '{
    "a": "abc",
    "d": "def",
    "z": [1, 2, 3]
}'::json;
        json        
--------------------
 {                 +
     "a": "abc",   +
     "d": "def",   +
     "z": [1, 2, 3]+
 }
(1 row)

select '{
    "a": "abc",
    "d": "def",
    "z": [1, 2, 3]
}'::jsonb;
                  jsonb                   
------------------------------------------
 {"a": "abc", "d": "def", "z": [1, 2, 3]}
(1 row)
</code></pre>

<p>Aqui nós vemos a remoção de espaços em branco. Eu diria que é muito legal. É claro, a não ser que (por qualquer motivo) você queira manter os espaços, mas isso não deveria ser significativo, então dependendo de eles estarem lá não parece sensato.</p>

<pre><code>select '{"a":"abc","d":"def","z":[1,2,3],"d":"overwritten"}'::json;
                        json                         
-----------------------------------------------------
 {"a":"abc","d":"def","z":[1,2,3],"d":"overwritten"}
(1 row)

select '{"a":"abc","d":"def","z":[1,2,3],"d":"overwritten"}'::jsonb;
                      jsonb                       
--------------------------------------------------
 {"a": "abc", "d": "overwritten", "z": [1, 2, 3]}
(1 row)
</code></pre>

<p>E o valor está sendo sobrescrito. Eu digo que isso é ótimo.</p>

<p>Quanto a utilização do espaço em disco, isso irá definitivamente depender do caso, mas um rápido teste mostra que jsonb pode utilizar bem mais espaço em disco:</p>

<pre><code>select pg_column_size('{"a":"abc","d":"def","z":[1,2,3]}'::jsonb);
 pg_column_size 
----------------
             84
(1 row)

select pg_column_size('{"a":"abc","d":"def","z":[1,2,3]}'::json);
 pg_column_size 
----------------
             37
(1 row)
</code></pre>

<p>Por outro lado, para este JSON:</p>

<pre><code>{"widget": {
    "debug": "on",
    "window": {
        "title": "Sample Konfabulator Widget",
        "name": "main_window",
        "width": 500,
        "height": 500
    },
    "image": { 
        "src": "Images/Sun.png",
        "name": "sun1",
        "hOffset": 250,
        "vOffset": 250,
        "alignment": "center"
    },
    "text": {
        "data": "Click Here",
        "size": 36,
        "style": "bold",
        "name": "text1",
        "hOffset": 250,
        "vOffset": 100,
        "alignment": "center",
        "onMouseUp": "sun1.opacity = (sun1.opacity / 100) * 90;"
    }
}}
</code></pre>

<p>O valor do JSON tem 605 bytes e o JSONb 524 bytes.</p>

<p>O que há de mais &hellip;</p>

<p>Para JSONb temos mais operadores. Por exemplo &ndash; igualdade:</p>

<pre><code>select '{"a":1, "b":2}'::jsonb = '{"b":2, "a":1}'::jsonb;
 ?column? 
----------
 t
(1 row)
</code></pre>

<p>Mais operadores estão descritos na <a href="http://www.postgresql.org/docs/devel/static/functions-json.html">documentação</a>.</p>

<p>O que mais &ndash; o novo tipo de dados pode utilizar índices para pesquisar por elementos.</p>

<p>Eu criei uma tabela para testes:</p>

<pre><code>create table test (v jsonb);
</code></pre>

<p>e inseri nela 100 mil linhas, parecidas com:</p>

<pre><code>{"i": 42, "s": "ryzdaoop"}
</code></pre>

<p>Algumas das linhas (~ 1%) tem um valor adicional no json &ndash; chave &ldquo;r&rdquo; com valor 1.</p>

<p>Agora eu posso criar um índice nesta tabela:</p>

<pre><code>create index whatever on test using gin (v);
CREATE INDEX
</code></pre>

<p>e agora:</p>

<pre><code>explain analyze select * from test where v ? 'r';
                                                      QUERY PLAN                                                       
-----------------------------------------------------------------------------------------------------------------------
 Bitmap Heap Scan on test  (cost=16.77..307.23 rows=100 width=42) (actual time=0.554..2.670 rows=1024 loops=1)
   Recheck Cond: (v ? 'r'::text)
   Heap Blocks: exact=644
   -&gt;  Bitmap Index Scan on whatever  (cost=0.00..16.75 rows=100 width=0) (actual time=0.416..0.416 rows=1024 loops=1)
         Index Cond: (v ? 'r'::text)
 Planning time: 0.475 ms
 Total runtime: 2.788 ms
(7 rows)
</code></pre>

<p>Eu também poderia fazer:</p>

<pre><code>explain analyze select * from test where v @&gt; '{"i":42}';
                                                      QUERY PLAN                                                      
----------------------------------------------------------------------------------------------------------------------
 Bitmap Heap Scan on test  (cost=28.77..319.23 rows=100 width=42) (actual time=1.132..1.707 rows=103 loops=1)
   Recheck Cond: (v @&gt; '{"i": 42}'::jsonb)
   Heap Blocks: exact=99
   -&gt;  Bitmap Index Scan on whatever  (cost=0.00..28.75 rows=100 width=0) (actual time=1.089..1.089 rows=103 loops=1)
         Index Cond: (v @&gt; '{"i": 42}'::jsonb)
 Planning time: 0.482 ms
 Total runtime: 1.783 ms
(7 rows)
</code></pre>

<p>Ou mesmo:</p>

<pre><code>explain analyze select * from test where v @&gt; '{"i":42, "r":1}';
                                                     QUERY PLAN                                                     
--------------------------------------------------------------------------------------------------------------------
 Bitmap Heap Scan on test  (cost=52.77..343.23 rows=100 width=42) (actual time=1.171..1.191 rows=3 loops=1)
   Recheck Cond: (v @&gt; '{"i": 42, "r": 1}'::jsonb)
   Heap Blocks: exact=3
   -&gt;  Bitmap Index Scan on whatever  (cost=0.00..52.75 rows=100 width=0) (actual time=1.143..1.143 rows=3 loops=1)
         Index Cond: (v @&gt; '{"i": 42, "r": 1}'::jsonb)
 Planning time: 0.530 ms
 Total runtime: 1.256 ms
(7 rows)
</code></pre>

<p>Isso é muito legal.</p>

<p>Infelizmente você não pode utilizar índices para realizar uma pesquisa mais &ldquo;profunda&rdquo;. O que significa isso?</p>

<p>Vamos supor que você tem um valor JSON como este:</p>

<pre><code>{"a": [1,2,3,4]}
</code></pre>

<p>Você pode indexá-lo, e pesquisar por &ldquo;value ? &lsquo;a&rsquo;&rdquo;, ou &ldquo;value @> &lsquo;{"a&rdquo;:[1,2,3,4]}&rsquo;&ldquo;, mas você não pode pesquisar (utilizando índice) por "linhas que tenha 3 na matriz que está sob a chave &lsquo;a&rsquo; no json&rdquo;.</p>

<p>É claro que você pode trabalhar em torno dele criando índice em ((value &ndash;> &lsquo;a&rsquo;)), se for isso que você realmente precisa.</p>

<p>De qualquer maneira eu realmente gostei disso. Parece que funciona muito bem, e eu espero que tenhamos mais recursos no futuro.</p>

<p>Valeu pessoal.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Projeto Aceito No Google Summer of Code 2014 (GSoC2014)]]></title>
    <link href="http://fabriziomello.github.io/blog/2014/04/26/projeto-aceito-no-google-summer-of-code-2014-gsoc2014/"/>
    <updated>2014-04-26T12:24:51-03:00</updated>
    <id>http://fabriziomello.github.io/blog/2014/04/26/projeto-aceito-no-google-summer-of-code-2014-gsoc2014</id>
    <content type="html"><![CDATA[<p>Amigos, gostaria de compartilhar um pouco de minha emoção e alegria, pois <a href="http://www.google-melange.com/gsoc/project/details/google/gsoc2014/fabriziomello/5738600293466112">minha proposta de projeto</a> foi aceita para o <a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2014">&ldquo;Google Summer of Code 2014&rdquo;</a>.</p>

<p>Nos próximos meses estarei desenvolvendo uma nova funcionalidade para o <a href="http://www.postgresql.org">PostgreSQL</a> financiado pelo <a href="http://www.google-melange.com/gsoc/homepage/google/gsoc2014">Google</a>, como projeto de pesquisa do curso de Pós-Graduação em <a href="http://www.uniritter.edu.br/pos/tecnologia/metodos_ageis">Tecnologias Aplicadas a Sistemas de Informação com Métodos Ágeis</a> pela <a href="http://www.uniritter.edu.br">Uniritter Porto Alegre/RS</a>.</p>

<p>O principal objetivo deste projeto é permitir que <a href="http://www.postgresql.org/docs/current/static/sql-createtable.html">&ldquo;unlogged tables&rdquo;</a> (tabelas que não geram registros no <a href="http://www.postgresql.org/docs/current/static/wal-intro.html">WAL</a>) sejam transformadas em tabelas regulares (que geram registros no WAL) e vice-versa. Para que isso aconteça será adicionado mais duas cláusulas ao comando sql &ldquo;ALTER TABLE&rdquo;:</p>

<pre><code>ALTER TABLE table_name SET LOGGED;
ALTER TABLE table_name SET UNLOGGED;
</code></pre>

<p>Quem tiver interesse em acompanhar a realização do projeto pode visitar a <a href="http://wiki.postgresql.org/wiki/Allow_an_unlogged_table_to_be_changed_to_logged_GSoC_2014">página do projeto no Wiki da comunidade PostgreSQL</a> e também o <a href="https://github.com/fabriziomello/postgres/tree/gsoc2014_alter_table_set_logged">código fonte no github</a>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Database Refactoring]]></title>
    <link href="http://fabriziomello.github.io/blog/2013/06/10/database-refactoring/"/>
    <updated>2013-06-10T00:00:00-03:00</updated>
    <id>http://fabriziomello.github.io/blog/2013/06/10/database-refactoring</id>
    <content type="html"><![CDATA[<div class='post'>
<h4><b>Contextualização</b></h4>Refatoração&nbsp;de código&nbsp;(<i><a href="https://en.wikipedia.org/wiki/Code_refactoring">Code Refactoring</a></i>) é uma disciplina/processo que consiste em melhorar a estrutura interna de um software sem modificar seu comportamento externo, e uma Refatoração de Banco de Dados (<i><a href="https://en.wikipedia.org/wiki/Database_refactoring">Database Refactoring</a></i>) parte do mesmo princípio, porém além de manter o comportamento externo também deve manter a semântica da informação que ele mantém/armazena, e por esse motivo é considerada mais difícil.<br /><br />Um outro conceito que posso destacar a respeito de <i>Database Refactoring</i> é:<br /><i>"Mudança disciplinada na estrutura de uma base de dados que não altera sua semântica, porém melhora seu projeto e minimiza a introdução de dados inconsistentes"</i><br /><br />O ponto interessante deste último é o texto "<i>minimiza a introdução de dados inconsistentes</i>", pois esse é o grande objetivo de realizarmos um <i>refactoring</i> na estrutura de um banco de dados, ou seja, melhorar o <i>desing</i>&nbsp;atual para melhorar a consistência dos dados e também a qualidade dos novos dados que serão adicionados ao seu banco de dados.<br /><br />E esta tarefa não é das mais simples, pois existe um fator preponderante no que diz respeito a dificuldade de execução deste tipo de <i>refactoring</i> que é o acoplamento, que será visto logo a seguir.<br /><h4><b>Acoplamento</b></h4><table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: right; text-align: center;"><tbody><tr><td style="text-align: center;"><span style="clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;"><a href="http://www.agiledata.org/essays/databaseRefactoring.html#Figure1BestCaseScenario"><img border="0" height="200" src="http://3.bp.blogspot.com/--JcWNVQiwEw/UbVRwiLB_pI/AAAAAAAAAag/DKCZt5EndPI/s200/dataRefactoringBestCase.gif" width="175" /></a></span></td></tr><tr><td class="tr-caption" style="text-align: center;"><a href="http://www.agiledata.org/essays/databaseRefactoring.html#Figure1BestCaseScenario">Figura 1. Baixo Acoplamento</a></td></tr></tbody></table>É a medida de dependência entre dois elementos. Quanto mais acoplados dois elementos estiverem, maior a chance que a mudança em um implique na mudança em outro.<br /><br /><table cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: left; text-align: left;"><tbody><tr><td style="text-align: center;"><span style="clear: left; margin-bottom: 1em; margin-left: auto; margin-right: auto;"><a href="http://www.agiledata.org/essays/databaseRefactoring.html#Figure2WorstCaseScenario"><img border="0" height="171" src="http://4.bp.blogspot.com/-G1MJ8-P0mPw/UbVRyTd6jFI/AAAAAAAAAas/LqwBNfBbVvc/s200/dataRefactoringWorstCase.gif" width="200" /></a></span></td></tr><tr><td class="tr-caption" style="text-align: center;"><a href="http://www.agiledata.org/essays/databaseRefactoring.html#Figure2WorstCaseScenario">Figura 2. Alto Acoplamento</a></td></tr></tbody></table>Simples assim, quanto mais o seu banco de dados estiver acoplado, ou seja, dependente de diversas aplicações externas, mais difícil será a aplicação de um <i>refactoring</i>.<br /><br />A Figura 1 demonstra um cenário "<i>Single-Database Application</i>" que é bem simplificado, onde a aplicação de um <i>refactoring</i>&nbsp;será mais tranquilo.<br /><br />Com certeza o cenário da Figura 2, o "<i>Multi-Database Application</i>"&nbsp;é o pior caso, pois exige muito cuidado e planejamento para execução do <i>refactoring</i>, então veremos a seguir uma sugestão de processo para execução.<br /><br /><h4><b>Processo de Refatoração</b></h4><table cellpadding="0" cellspacing="0" class="tr-caption-container" style="clear: right; float: right; margin-bottom: 1em; text-align: right;"><tbody><tr><td style="text-align: center;"><span style="clear: right; margin-bottom: 1em; margin-left: auto; margin-right: auto;"><a href="http://www.agiledata.org/essays/databaseRefactoring.html#Figure3Process"><img border="0" height="320" src="http://2.bp.blogspot.com/-GuIfZIWdI9M/UbVNf7VP42I/AAAAAAAAAaQ/IuYaQ9S4wPk/s320/databaseRefactoringProcess.gif" width="252" /></a></span></td></tr><tr><td class="tr-caption" style="text-align: center;">Figura 3. Processo de&nbsp;<i>Database Refactoring</i></td></tr></tbody></table>Um processo é um conjunto organizado de atividades com um objetivo em comum. Executar um&nbsp;<i>database refactoring</i>&nbsp;em um cenário "<i>Single-Database Application</i>" ou "<i>Multi-Application Database</i>" requer um processo, por mais simples que seja. A grande diferença na execução em ambos cenários é que no caso do "<i>Multi-Application Database</i>" o período de transição (mais abaixo falaremos) geralmente será mais longo.<br /><br />É bom sempre ter em mente que um <i>database refactoring</i>, como já vimos,&nbsp;não é uma atividade simples então caso seja identificada a real necessidade de refatorar um banco de dados então podemos usar o seguinte roteiro (processo) para se guiar:<br /><ul><li>Escolher o <i>refactoring</i> mais apropriado;</li><li>Depreciar o esquema original;</li><li>Testar antes, durante e após;</li><li>Modificar esquema;</li><li>Migrar os dados;</li><li>Modificar código externo;</li><li>Executar testes de regressão;</li><li>Versionar seu trabalho;</li><li>Anunciar o refactoring.</li></ul><div><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="float: right; margin-left: 1em; text-align: right;"><tbody><tr><td style="text-align: center;"><a href="http://1.bp.blogspot.com/-jEsZQM4gxVU/UbVWvbD4CsI/AAAAAAAAAa8/7VDnxsuE6E0/s1600/process-refactoring-regra-geral.jpg" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" height="88" src="http://1.bp.blogspot.com/-jEsZQM4gxVU/UbVWvbD4CsI/AAAAAAAAAa8/7VDnxsuE6E0/s320/process-refactoring-regra-geral.jpg" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Figura 4. Regra Geral Processo Refatoração</td></tr></tbody></table><br />Na Figura 4 é demonstrado um pequeno processo descrevendo um fluxo básico para aplicação de um <i>refactoring</i>.<br /><br />Atente bem para o "Período de Transição", que é a fase mais importante, principalmente para cenários "<i>Multi-Database Application</i>" (Figura 2), onde você precisa ter em mente que não conseguirá realizar o <i>refactoring</i>&nbsp;e fazer o <i>deploy</i> em produção de todas as aplicações ao mesmo tempo. Na verdade você nem conseguirá alterar todas as aplicações ao mesmo tempo, principalmente se você tiver dependência de terceiros, então você precisará suportar o esquema original e o esquema resultante ao mesmo tempo, para somente quando todas aplicações estiverem suportando apenas o esquema resultante, ou novo esquema, você poderá aposentar de vez o antigo esquema e assim finalizar este período.<br /><br /><h4><b>Estratégias de <i>Database Refactorings</i></b></h4>Existem alguns pontos a considerar com estratégias para adoção de um <i>database refactoring</i>:<br /><br /><ul><li>Pequenas mudanças são mais fáceis de aplicar;</li><li>Identifique unicamente cada <i>refactoring</i>;</li><li>Implemente uma grande mudança realizando várias pequenas mudanças;</li><li>Tenha uma tabela de configuração/versionamento do seu banco de dados;</li><li>Priorize <i>triggers</i>&nbsp;ao invés de <i>views</i>&nbsp;ou sincronizações em lote;</li><li>Escolha um período de transição suficiente para realizar as mudanças;</li><li>Simplifique sua estratégia de controle de versão de banco de dados;</li><li>Simplifique negociações com outros times;</li><li>Encapsule acesso ao banco de dados;</li><li>Habilite-se a montar facilmente um ambiente de banco de dados;</li><li>Não duplique SQL;</li><li>Coloque os ativos de banco de dados sobre controle de mudanças;</li><li>Seja cuidadoso com políticas.</li></ul><div>Os items acima mostram apenas algumas sugestões, em forma de "lições aprendidas", de algumas estratégias que você pode considerar quando tiver a necessidade de realizar um <i>refactoring</i>.</div><div>Para apoiar essas estratégias existe um catálogo que descrevem diversos tipos de <i>refactorings</i>&nbsp;em bancos de dados&nbsp;e exemplos de uso, que veremos a seguir.</div><div><br /></div><h4><b><a href="http://www.agiledata.org/essays/databaseRefactoringCatalog.html">Catálogo de&nbsp;<i>Database Refactorings</i></a></b></h4><div>Este catálogo é dividido em algumas categorias:<br /><br /><ul><li><i><a href="http://www.agiledata.org/essays/databaseRefactoringCatalogStructural.html">Structural</a></i>: são mudanças na estrutura do banco de dados (tabelas, colunas, visões, etc).</li><li><i><a href="http://www.agiledata.org/essays/databaseRefactoringCatalogDataQuality.html">Data Quality</a></i>: são mudanças que melhoram a qualidade das informações contidas em um banco de dados.</li><li><i><a href="http://www.agiledata.org/essays/databaseRefactoringCatalogReferentialIntegrity.html">Referential Integrity</a></i>: são mudanças que asseguram que uma linha referenciada exista em outra relação e/ou assegura que uma linha que não é mais necessária seja removida apropriadamente.</li><li><i><a href="http://www.agiledata.org/essays/databaseRefactoringCatalogArchitectural.html">Architectural</a></i>: são mudanças que melhoram a maneira que programas externos interagem com a base de dados.</li><li><i><a href="http://www.agiledata.org/essays/databaseRefactoringCatalogMethod.html">Method</a></i>: são mudanças que melhoram a qualidade de uma Procedure um Função.</li><li><i><a href="http://www.agiledata.org/essays/databaseRefactoringCatalogTransformations.html">Transformations</a></i>: mudanças que alteram a semântica do esquema do banco pela adição de novas funcionalidades.</li></ul><br /><div>No meu <a href="https://github.com/fabriziomello/database_refactoring">github</a> é possível encontrar exemplos práticos de aplicação passo-a-passo de um refactoring em um <a href="https://github.com/fabriziomello/database_refactoring/blob/master/model/01_modelo_inicial.png">modelo inicial</a>, passando por um <a href="https://github.com/fabriziomello/database_refactoring/blob/master/model/02_modelo_fks.png">período de transição</a> e chegando ao <a href="https://github.com/fabriziomello/database_refactoring/blob/master/model/03_modelo_city.png">modelo final</a>.&nbsp;</div><div><br /></div><div><h4>Considerações Finais</h4>Devemos levar em consideração que apesar destas técnicas serem direcionadas para refatoração, ou seja, mudar estrutura sem mudar sua semântica, as mesmas podem e devem ser utilizadas para evolução da sua aplicação, ou seja, se você precisa construir uma nova <i>feature</i>&nbsp;em sua aplicação que está em produção, você poderá recorrer das práticas aqui apresentadas para evoluir seu esquema de forma mais consistente e segura.<br /><br />Baseado no exposto podemos facilmente responder a pergunta "Por quê Refatorar?":<br /><br /><ul><li>aceitar mudança de escopo;</li><li>fornecer feedback rápido;</li><li>melhoria contínua;</li><li>aumentar simplicidade para facilitar entendimento;</li><li>tornar os modelos mais próximos do mundo real;</li><li>termos modelos simples para facilitar:</li><ul><li>manutenção e</li><li>evolução da aplicação</li></ul></ul>E para refatorarmos precisamos ter conhecimento, disciplina, simplicidade, bom senso e persistência, sem contar no ponto fundamental que é organização.<br /><br /><h4>Referências</h4><ul><li>Livros:&nbsp;</li><ul><li><a href="http://www.ambysoft.com/books/refactoringDatabases.html">Refactoring Databases</a> (Scott W. Ambler e Pramod J. Sadalage)</li><li><a href="http://www.ambysoft.com/books/agileDatabaseTechniques.html">Agile Database Techniques</a> (Scott W. Ambler)</li><li><a href="http://martinfowler.com/books/refactoring.html">Refactoring</a> (Martin Fowler)</li><li><a href="http://www.ambysoft.com/books/agileModeling.html">Agile Modeling</a> (Scott W. Ambler)&nbsp;</li></ul><li>Links:</li><ul><li><a href="http://www.agiledata.org/">http://www.agiledata.org</a></li><li><a href="http://www.databaserefactoring.org/">http://www.databaserefactoring.org</a></li><li><a href="http://visaoagil.wordpress.com/">http://visaoagil.wordpress.com</a></li><li><a href="http://www.refactoring.com/">http://www.refactoring.com</a></li><li><a href="http://www.postgresql.org/">http://www.postgresql.org</a></li><li><a href="http://www.epictest.org/">http://www.epictest.org</a></li><li><a href="http://www.slideshare.net/diogobiazus/testes-unitarios-com-postgre-sql">http://www.slideshare.net/diogobiazus/testes-unitarios-com-postgre-sql</a></li></ul><li>Slides</li><ul><li><a href="http://www.slideshare.net/fabriziomello/tag/refactoring">http://www.slideshare.net/fabriziomello/tag/refactoring</a></li><li><a href="http://www.slideshare.net/antonkeks/database-refactoring">http://www.slideshare.net/antonkeks/database-refactoring</a></li></ul></ul></div><ul></ul></div></div><div class="separator" style="clear: both; text-align: center;"></div></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PostgreSQL 9.3devel - Nova Extensão "Postgres_fdw"]]></title>
    <link href="http://fabriziomello.github.io/blog/2013/02/21/postgresql-93devel-nova-extensao/"/>
    <updated>2013-02-21T00:00:00-03:00</updated>
    <id>http://fabriziomello.github.io/blog/2013/02/21/postgresql-93devel-nova-extensao</id>
    <content type="html"><![CDATA[<div class='post'>
Apartir da <a href="http://www.postgresql.org/docs/9.1/static/release-9-1.html">release 9.1</a> o PostgreSQL adicionou suporte a <a href="http://www.postgresql.org/docs/9.1/static/sql-createforeigntable.html">foreign tables</a>, viabilizando assim uma forma simples de <a href="http://en.wikipedia.org/wiki/SQL/MED">gerenciar fontes de dados externas</a> dentro do <a href="http://wiki.postgresql.org/wiki/SQL/MED">PostgreSQL</a>.<br /><br />Através dessa infraestrutura é possível a <a href="http://www.postgresql.org/docs/9.2/static/fdwhandler.html">implementação dos FDW (Foreign Data Wrapper)</a>, que são uma espécie de driver para acessar uma fonte de dados externa. <br /><br />Já existem <a href="http://wiki.postgresql.org/wiki/Foreign_data_wrappers">diversos FDW implementados</a> que permitem acessar outros bancos de dados (oracle, mysql, etc), arquivos (texto, csv, etc), bases NoSQL (mongodb, couchdb, redis, etc) e outras fontes de dados diferentes, tais como: twitter, ldap, www, etc.<br /><br />Hoje foi commitado no git do PostgreSQL um FDW específico para acessar bases PostgreSQL, chamado "<a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=commit;h=d0d75c402217421b691050857eb3d7af82d0c770">postgres_fdw</a>", então vou demonstrar como instalar/configurar de uma forma muito simples.<br /><br /><br /><b>1) Instalar o "postgres_fdw" apartir do git</b><br /><br />Nesse exemplo vou mostrar como instalar/compilar o PostgreSQL apartir do <a href="http://git.postgresql.org/gitweb/?p=postgresql.git;a=summary">git oficial</a>, mas se vc tiver uma conta no <a href="https://github.com/">github</a> pode usar tb o nosso <a href="https://github.com/postgres/postgres">clone do repositório oficial</a>.<br /><br /><pre class="brush: bash; toolbar: false">git clone git://git.postgresql.org/git/postgresql.git<br />cd postgresql<br />./configure --prefix=$HOME/pgsql<br />make<br />make install<br />cd contrib/postgres_fdw<br />make<br />make install<br /></pre><br /><br /><b>2) Inicializar um novo cluster e colocar o PostgreSQL para executar</b><br /><br /><pre class="brush: bash; toolbar: false">$ cd $HOME/pgsql<br />$ mkdir data<br />$ chmod 700 data<br />$ ./bin/initdb -D data <br />$ ./bin/pg_ctl -D data -l startup.log start<br /></pre><br /><br /><b>3) Criar bases e tabela para testes</b><br /><br /><pre class="brush: bash; toolbar: false">$ cd $HOME/pgsql<br />$ ./bin/createdb bd1<br />$ ./bin/createdb bd2<br />$ ./bin/psql bd2 -c "create table foo(bar integer);"<br />$ ./bin/psql bd2 -c "insert into foo(bar) select * from generate_series(1, 10);"<br /></pre><br />Para explicar, criamos 2 (duas) bases de dados "bd1" e "bd2", e no "bd2" criamos uma tabela chamada "foo" a qual iremos acessar apartir do "bd1" criando uma foreign table como explicarei em seguida.<br /><br /><br /><b>4) Criar a extensão "postgres_fdw" no "bd1"</b><br /><br /><pre class="brush: sql; toolbar: false">$ ./bin/psql bd1<br />psql (9.3devel)<br />Type "help" for help.<br /><br />bd1=# CREATE EXTENSION postgres_fdw ;<br />CREATE EXTENSION<br /><br />bd1=# \dx<br />                               List of installed extensions<br />     Name     | Version |   Schema   |                    Description                     <br />--------------+---------+------------+----------------------------------------------------<br /> plpgsql      | 1.0     | pg_catalog | PL/pgSQL procedural language<br /> postgres_fdw | 1.0     | public     | foreign-data wrapper for remote PostgreSQL servers<br />(2 rows)<br /></pre><br /><br /><b>5) Criar conexão com "bd2" no "bd1"</b><br /><br /><pre class="brush: sql; toolbar: false">bd1=# CREATE SERVER conexao_bd2 FOREIGN DATA WRAPPER postgres_fdw OPTIONS (dbname 'bd2');<br />CREATE SERVER<br /><br />bd1=# \des+<br />                                              List of foreign servers<br />    Name     |  Owner   | Foreign-data wrapper | Access privileges | Type | Version |  FDW Options   | Description <br />-------------+----------+----------------------+-------------------+------+---------+----------------+-------------<br /> conexao_bd2 | fabrizio | postgres_fdw         |                   |      |         | (dbname 'bd2') | <br />(1 row)<br /><br />bd1=# CREATE USER MAPPING FOR current_user SERVER conexao_bd2 ;<br />CREATE USER MAPPING<br /><br />bd1=# \deu+<br />         List of user mappings<br />   Server    | User name | FDW Options <br />-------------+-----------+-------------<br /> conexao_bd2 | fabrizio  | <br />(1 row)<br /></pre><br /><br /><br /><b>6) Acessar a tabela "foo" do "bd2" apartir do "bd1"</b><br /><br /><pre class="brush: sql; toolbar: false">bd1=# CREATE FOREIGN TABLE foo (bar integer) SERVER conexao_bd2 ;<br />CREATE FOREIGN TABLE<br /><br />bd1=# \d<br />            List of relations<br /> Schema | Name |     Type      |  Owner   <br />--------+------+---------------+----------<br /> public | foo  | foreign table | fabrizio<br />(1 row)<br /><br />bd1=# SELECT * FROM foo;<br /> bar <br />-----<br />   1<br />   2<br />   3<br />   4<br />   5<br />   6<br />   7<br />   8<br />   9<br />  10<br />(10 rows)<br /></pre><br /><br /><b>Considerações</b><br /><br />Segundo a própria documentação oficial, o "<a href="http://www.postgresql.org/docs/devel/static/postgres-fdw.html">postgres_fdw</a>" é uma alternativa mais robusta em relação ao antigo "<a href="http://www.postgresql.org/docs/current/static/dblink.html">dblink</a>" pois nos oferece uma sintaxe mais padronizada e simplificada para acessar tabelas remotas, inclusive com melhor desempenho em muitos casos. <br /><br />E pelo que pude acompanhar do seu desenvolvimento, parece que essa FDW deve servir como modelo para o desenvolvimento de outras FDW para acessar outras bases de dados relacionais.<br /><br />De uma forma muito simples é possível acessar tabelas de outra base de dados PostgreSQL, e o mesmo ocorre com outras fontes de dados, mas por enquanto apenas para leitura (SELECT), entretanto já está em revisão um <a href="https://commitfest.postgresql.org/action/patch_view?id=919">patch</a> para permitir escrita (INSERT/UPDATE/DELETE) em foreign tables, vamos aguardar.</div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PostgreSQL + Unlogged Tables + Partitioning + Parallel Programming = ETL Reescrito Passando De ~8h Para ~25min De Execução]]></title>
    <link href="http://fabriziomello.github.io/blog/2013/02/20/postgresql-unlogged-tables-partitioning/"/>
    <updated>2013-02-20T00:00:00-03:00</updated>
    <id>http://fabriziomello.github.io/blog/2013/02/20/postgresql-unlogged-tables-partitioning</id>
    <content type="html"><![CDATA[<div class='post'>
Já faz algum tempo que não escrevo nada por aqui, mas não é por falta de tempo ou coisa parecida, é que na realidade não tenho muita intimidade com artigos ou posts em blogs, mas resolvi escrever um "causo" a pedido do amigo <a href="http://www.fernandoike.com/">Fernando Ike</a> sobre um <a href="https://twitter.com/fabriziomello/status/295317858177658881">tweet</a> que lancei há algum tempo depois de obter sucesso em um projeto.<br /><br />Aviso antecipadamente que o post é um pouco longo, então se não estiver com paciência agora recomendo vc sair tomar um café (ou uma cerveja) e voltar outra hora... desculpe mesmo, tentei reduzir o máximo... :-(<br /><br /><h4>Contextualização</h4>Para vcs entenderem porque cheguei aqui, vou começar dos primórdios... eu tenho (ou tinha...hehehe...) um problema com um ETL em uma aplicação de um cliente (não tenho autorização para "dar nome aos bois") que basicamente processava os registros de uma grande tabela com dados financeiros e gerava uma "posição" da mesma calculando correção monetária, juros, multas, descontos, etc...<br /><br />Esse ETL sempre foi e ainda é ridiculamente simples, porque basicamente é uma PL/pgSQL dentro do PostgreSQL que faz todo esse trabalho de ler os dados de uma tabela, processar e carregar os mesmos em outra tabela. Até aqui tudo bem, sempre funcionou maravilhosamente bem, mas com bases pequenas... mas também não queremos maravilhas de desempenho processando milhares de registros em uma única transação né, é óbvio que isso gera problemas.<br /><br /><h4>Primeira tentativa... e um sucesso, digamos, opaco...</h4>Há alguns anos eu já tinha melhorado essa rotina dividindo o processamento em lotes menores, através de um shell script que fazia esse trabalho de divisão em lotes por uma coluna da tabela que categorizava os registros em um determinado tipo, então primeiro identificamos os tipos existentes na tabela origem e processava os mesmos gerando tabelas individuais para cada tipo, e ao final juntava tudo na tabela de destino e removia as tabelas temporárias... e vejam só, SEMPRE a MESMA tabela de destino, e para isso precisamos remover os índices, executar o ETL e depois criar novamente os índices para termos um desempenho decente. Claro que junto dessa rotina implementamos também uma outra para expurgo de registros desnecessários/obsoletos (antigos), o que também sempre foi uma rotina que onerava bastante o servidor pois era SEMPRE a MESMA tabela de destino, então imaginem precisar remover uma porção de registros de uma tabela com mais de 100milhões de registros... isso me lembra um post do <a href="http://tellesr.com/">Fábio Telles</a>: <a href="http://savepoint.blog.br/nao-use-delete-use-insert/">"Não use DELETE use INSERT"</a>&nbsp;que ajudou muito para torná-la "menos pior".<br /><br />Na época (2007/2008) essa melhoria ajudou pois desafogou bastante a carga de processamento desse ETL, porém com o passar do tempo e a tabela com dados financeiros crescendo constantemente, o ETL foi ficando cada vez mais oneroso chegando ao seu ápice (final de 2012) de ~8h de execução para processar ~8.5milhões de registros. Eu sei que esse número não é tão expressivo assim, mas a complexidade do processamento envolvido para fazer os cálculos de corrreção, juros e multas e as diversas configurações existentes para cada um justificam, de certa forma, todo esse tempode processamento, sem contar que o coitado do servidor ficava "imprestável".<br /><br />Mas vejam bem, estou falando de dados *financeiros* que sob ponto de vista do negócio se fazem muito necessários para vários tipos de procedimentos e análises. Ainda existem instâncias com bases menores (outros clientes) que rodam esse ETL _diariamente_ por necessidade de negócio, mas nesse cliente em especial não é possível fazer isso, nem que eles quisessem &nbsp;pois o tempo total de execução consume 1/3 de um dia, então os finais de semana são usados :-)<br /><br /><h4>A hora da verdade ...</h4>Após todos os problemas e sem muitas perspectivas, discutimos sobre a re-implementação da PL/pgSQL que executava o ETL, porém isso não é algo trivial, ainda mais em um ERP complexo onde tal iniciativa teria um impacto de grandes proporções visto que seria necessária uma re-modelagem em &nbsp;alguns pontos criticos. Apesar de ser uma idéia interessante, &nbsp;não existe tempo hábil para tanto, pois o cliente não pode mais aguardar uma solução, pois qto mais tempo demorar pior fica.<br /><br />Como eu já tenho algum tempo de estrada com PostgreSQL e conheço bem a estrtutura do ETL e do ERP em questão, sugeri a equipe que eu poderia re-implementar o antigo shell script reaproveitando a PL/pgSQL do ETL existente (sem mudar regras de negócio), usando tecnologias e técnicas conhecidas. Então o que fiz:<br /><br />1) Particionamento da tabela: esse foi o ponto fundamental, pois dividimos a grande tabela em outras menores tomando como base uma coluna que indica a "data" em que os dados financeiros foram calculados, e que o ERP usa constantemente para ler informações da mesma, portanto as queries iriam se beneficiar do recurso. Sobre esse assunto, além da <a href="http://www.postgresql.org/docs/current/static/ddl-partitioning.html">documentação oficial</a>, vcs podem dar uma olhada em <a href="http://savepoint.blog.br/?s=particionamento&amp;submit=Pesquisa">alguns artigos</a> recém lançados pelo <a href="http://tellesr.com/">Fábio Telles</a> sobre esse assunto.<br /><br />2) Implementação de um script PHP (não estou de sacanagem... é PHP mesmo, mas no console) que tivesse a habilidade de gerar processos filhos (fork) para processamento em paralelo, e para isso usei uma <a href="http://blog.motane.lu/2009/01/02/multithreading-in-php/">classe para realizar esse trabalho</a>. Confesso que no inicio tive um certo receio em implementar essa rotina em PHP, inclusive cogitei a possibilidade de fazê-la em Perl, Python ou Ruby, mas como eu domino mais esta do que as outras e o tempo era curto implementei nela mesmo, e os resultados foram muito satisfatórios.<br /><br /><br /><h4>COPY no lugar de INSERT</h4>A primeira coisa que fiz para continuar esse projeto foi *abolir* o INSERT... isso mesmo... não tem INSERT... vc deve estar pensando que estou maluco e se perguntando: "Tá e como adicionar linhas a uma tabela então?" R: usando <a href="http://www.postgresql.org/docs/current/static/sql-copy.html">COPY</a>, ao invés de INSERT... na realidade implementei uma classe que armazena uma coleção (linhas) em memória, e quando eu preciso uso um método para persistir os dados em uma tabela usando COPY... simples assim... então o código usado para INSERT é algo do tipo:<br /><br />DDL da tabela exemplo:<br /><pre class="brush: sql; toolbar: false">create table foo (<br />  bar integer<br />);<br /></pre><br />PHP:<br /><pre class="brush: php; toolbar: false">$tabela = new PgCopy('foo');<br />for ($i=0; $i<10; $i++) {<br />    $tabela->bar = $i;<br />    $tabela->insertValue(); // adiciona em memória<br />}<br />$tabela->persist(); // realiza COPY dos dados em memória<br /></pre><br /><br /><h4>Dividir para conquistar</h4>Um dos problemas que tinhamos com o processo antigo era justamente que ele era linear, ou seja, um processo apenas com inicio, meio e fim. Então resolvi investir em programação paralela, dividindo o grande volume de registros a processar em vários trabalhos menores sendo capaz de executar alguns em paralelo, de acordo com o nro de núcleos do servidor.<br /><br />Para tal atividades crio uma tabela que <b>planeja</b> a execução do trabalho, ou seja, cria lotes para que o script possa processar em paralelo, isso baseado em uma chave artifical (sequencial) que existe no modelo e facilitou a criação de trabalhos com lotes de N registros (neste caso usei 1000).<br /><br />A tabela que crio para planejar a execução do ETL é algo do tipo:<br /><pre class="brush: sql; toolbar: false">create table jobs (<br />  id_start bigint,<br />  id_end bigint,<br />  status varchar,<br />  constraint jobs_pk primary key (id_start, id_end),<br />  constraint jobs_status_ck check (status in ('NOT RUNNING', 'RUNNING', 'FINISHED'))<br />);<br /></pre><br />Dessa forma utilizo o "id_start" e "id_end" para buscar as informações na origem em "lotes" de 1000 (mil) registros, e com isso consigo disparar vários processos em paralelo, e dessa forma conseguimos aproveitar melhor os recursos do servidor e assim agilizar bastante o processo.<br /><br /><br /><h4>Unlogged Tables são bem legais</h4>Esse <a href="http://www.postgresql.org/docs/current/static/sql-createtable.html">novo recurso</a> presente apartir da <a href="http://www.postgresql.org/docs/current/interactive/release-9-1.html">versão 9.1</a> permite criar tabelas que não são escritas no <a href="http://www.postgresql.org/docs/current/static/wal.html">log de transações (WAL)</a>, acelerando e muito a inserção de registros na mesma.<br /><br />Assim cada processo disparado pelo script gera e escreve em uma unlogged table os dados, e junto com os processos de trabalho (workers) implementei um processo especial que serve com um tipo de coletor de lixo (garbage collector) para ir gradativamente lendo os lotes processados (unlogged tables geradas) e inserindo (com copy claro) na partição de destino.<br /><br />Com essa estratégia posso ter um certo nível de escala na escrita pois consigo separar as tabelas em tablespaces distintas. Claro que se algum imprevisto ocorrer, tipo um desligamento não previsto do servidor, o próprio script tem habilidade de detectar essa situação e fazer uma limpeza geral antes de inicar um novo processo, até mesmo porque as unlogged tables tem seu conteúdo eliminado nessas situações, e não queremos perder parte dos registros não é mesmo... :-)<br /><br /><h4>Finalizando...</h4>Resumindo o que fiz foi:<br />- Particionar uma tabela grande em outras menores<br />- Planejar o processamento dividindo em lotes menores para poder fazer processamento paralelo<br />- Utilizar unlogged tables para receber os dados oriundos dos lotes que são processados em paralelo<br />- Implementar um processo que irá ler os lotes já processados e inserir os registros na partição de destino.<br /><br />Existem outras coisas que foram feitas para melhorar o desempenho, tipo desligar o autovacuum nas tabelas, aumentar o work_mem, criar índices necessários ao final do processamento, e outros que podem ser feitos e que vcs podem visualizar neste <a href="http://savepoint.blog.br/acelerando-a-importacao-de-dados-no-postgresql/">post do Fábio</a>.<br /><br />Bom, se vc chegou até aqui então obrigado pela paciência e se quiser mais informações fico a disposição.<br /><br /><br />---<br />Fabrízio de Royes Mello<br />fabriziomello [at] gmail.com<br /></div>


<h2>Comments</h2>


<div class='comments'>
<div class='comment'>
<div class='author'>Fabrízio de Royes Mello</div>
<div class='content'>
Há mto tempo atrás (Ago/2009), segui as instruções contidas em [1] para se cadastrar no Planeta (enviei email e td mais), porém nada aconteceu... nenhuma resposta, retorno, nada... :-(<br /><br /><br />[1] http://wiki.postgresql.org.br/PlanetaPostgreSQLBR</div>
</div>
<div class='comment'>
<div class='author'>Unknown</div>
<div class='content'>
O artigo ficou muito massa. Só tem uma coisa que me preocupa aqui.... qual o motivo do seu blog ainda não estar no nosso planeta.postgresql.org.br ???</div>
</div>
</div>

]]></content>
  </entry>
  
</feed>
